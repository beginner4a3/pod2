{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéôÔ∏è Unified Podcast Generator\n",
                "\n",
                "## Before Starting:\n",
                "1. **Enable GPU**: `Runtime` ‚Üí `Change runtime type` ‚Üí `T4 GPU`\n",
                "2. **Get HuggingFace Token**: https://huggingface.co/settings/tokens\n",
                "3. **Request Model Access**: https://huggingface.co/ai4bharat/indic-parler-tts (click 'Request access')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install (then RESTART runtime!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repo\n",
                "!git clone https://github.com/beginner4a3/pod2.git\n",
                "%cd pod2\n",
                "\n",
                "# ‚ö†Ô∏è IMPORTANT: Replace with YOUR actual token!\n",
                "HF_TOKEN = \"hf_xxxxxxxxxxxxxxxxxxxx\"  # <-- PUT YOUR TOKEN HERE\n",
                "\n",
                "# Save token for after restart\n",
                "with open('/content/hf_token.txt', 'w') as f:\n",
                "    f.write(HF_TOKEN)\n",
                "\n",
                "# Install numpy/scipy first (pinned for compatibility)\n",
                "!pip install numpy==1.26.4 scipy==1.12.0 -q\n",
                "\n",
                "# Install main packages\n",
                "!pip install parler-tts transformers huggingface_hub -q\n",
                "!pip install soundfile librosa pydub gradio fastapi uvicorn -q\n",
                "!pip install PyPDF2 python-docx python-multipart -q\n",
                "\n",
                "# Install llama-cpp-python WITH CUDA (from pre-built wheels)\n",
                "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 -q\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"‚ö†Ô∏è  RESTART RUNTIME NOW!\")\n",
                "print(\"=\"*50)\n",
                "print(\"Go to: Runtime ‚Üí Restart runtime\")\n",
                "print(\"Then run Step 2.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Download Models (after restart)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%cd /content/pod2\n",
                "import os\n",
                "\n",
                "# Load token\n",
                "with open('/content/hf_token.txt') as f:\n",
                "    os.environ['HF_TOKEN'] = f.read().strip()\n",
                "\n",
                "# Verify token\n",
                "token = os.environ['HF_TOKEN']\n",
                "if token.startswith('hf_') and len(token) > 10:\n",
                "    print(f\"‚úÖ Token: {token[:10]}...\")\n",
                "else:\n",
                "    print(\"‚ùå Invalid token! Put your real token in Step 1.\")\n",
                "\n",
                "# Download models\n",
                "!python setup_models.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Launch UI üöÄ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '/content/pod2')\n",
                "\n",
                "from src.ui.gradio_app import create_interface\n",
                "\n",
                "demo = create_interface()\n",
                "demo.queue().launch(share=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Quick Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.tts.indic_parler import IndicParlerTTS\n",
                "from IPython.display import Audio\n",
                "\n",
                "tts = IndicParlerTTS()\n",
                "audio = tts.generate(\"‡§®‡§Æ‡§∏‡•ç‡§§‡•á!\", speaker=\"Rohit\")\n",
                "tts.save(audio, \"test.wav\")\n",
                "Audio(\"test.wav\")"
            ]
        }
    ]
}